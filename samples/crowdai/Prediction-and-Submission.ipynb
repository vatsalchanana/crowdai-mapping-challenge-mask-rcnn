{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mapping-challenge-mask_rcnn-prediction-submission\n",
    "![CrowdAI-Logo](https://github.com/crowdAI/crowdai/raw/master/app/assets/images/misc/crowdai-logo-smile.svg?sanitize=true)\n",
    "\n",
    "This notebook contains the code for making predictions from the model trained in [Training.ipynb](Training.ipynb) (or by using the [released pretrained model](https://www.crowdai.org/challenges/mapping-challenge/dataset_files)) for the [crowdAI Mapping Challenge](https://www.crowdai.org/challenges/mapping-challenge).\n",
    "\n",
    "This code is adapted from the [Mask RCNN]() tensorflow implementation available here : [https://github.com/matterport/Mask_RCNN](https://github.com/matterport/Mask_RCNN).\n",
    "\n",
    "First we begin by importing all the necessary dependencies : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import skimage.io\n",
    "\n",
    "# Download and install the Python COCO tools from https://github.com/waleedka/coco\n",
    "# That's a fork from the original https://github.com/pdollar/coco with a bug\n",
    "# fix for Python 3.\n",
    "# I submitted a pull request https://github.com/cocodataset/cocoapi/pull/50\n",
    "# If the PR is merged then use the original repo.\n",
    "# Note: Edit PythonAPI/Makefile and replace \"python\" with \"python3\".\n",
    "#  \n",
    "# A quick one liner to install the library \n",
    "# !pip install git+https://github.com/waleedka/coco.git#subdirectory=PythonAPI\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "from pycocotools import mask as maskUtils\n",
    "\n",
    "import coco #a slightly modified version\n",
    "\n",
    "from evaluate import build_coco_results, evaluate_coco\n",
    "from dataset import MappingChallengeDataset\n",
    "from mrcnn import visualize\n",
    "\n",
    "\n",
    "import zipfile\n",
    "import urllib.request\n",
    "import shutil\n",
    "import glob\n",
    "import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset location \n",
    "Now we expect that you have downloaded all the files in the datasets section and untar-ed them to have the following structure :\n",
    "```\n",
    "├── data\n",
    "|   ├── pretrained_weights.h5 (already included in this repository)\n",
    "│   ├── test\n",
    "│   │   └── images/\n",
    "│   │   └── annotation.json\n",
    "│   ├── train\n",
    "│   │   └── images/\n",
    "│   │   └── annotation.json\n",
    "│   └── val\n",
    "│       └── images/\n",
    "│       └── annotation.json\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"../../\")\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import model as modellib, utils\n",
    "\n",
    "\n",
    "PRETRAINED_MODEL_PATH = os.path.join(ROOT_DIR,\"data/\" \"pretrained_weights.h5\")\n",
    "LOGS_DIRECTORY = os.path.join(ROOT_DIR, \"logs\")\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "IMAGE_DIR = os.path.join(ROOT_DIR, \"data\", \"test\", \"images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantitate Inference Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceConfig(coco.CocoConfig):\n",
    "    # Set batch size to 1 since we'll be running inference on\n",
    "    # one image at a time. Batch size = GPU_COUNT * IMAGES_PER_GPU\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 5\n",
    "    NUM_CLASSES = 1 + 1  # 1 Background + 1 Building\n",
    "    IMAGE_MAX_DIM=320\n",
    "    IMAGE_MIN_DIM=320\n",
    "    NAME = \"crowdai-mapping-challenge\"\n",
    "config = InferenceConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = modellib.MaskRCNN(mode=\"inference\", model_dir=MODEL_DIR, config=config)\n",
    "\n",
    "model_path = PRETRAINED_MODEL_PATH\n",
    "\n",
    "# or if you want to use the latest trained model, you can use : \n",
    "# model_path = model.find_last()[1]\n",
    "\n",
    "model.load_weights(model_path, by_name=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Prediction on a single Image (and visualize results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['BG', 'building'] # In our case, we have 1 class for the background, and 1 class for building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = next(os.walk(IMAGE_DIR))[2]\n",
    "random_image = skimage.io.imread(os.path.join(IMAGE_DIR, random.choice(file_names)))\n",
    "\n",
    "predictions = model.detect([random_image]*config.BATCH_SIZE, verbose=1) # We are replicating the same image to fill up the batch_size\n",
    "\n",
    "p = predictions[0]\n",
    "visualize.display_instances(random_image, p['rois'], p['masks'], p['class_ids'], \n",
    "                            class_names, p['scores'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run predictions on all the images in the test set\n",
    "\n",
    "Note that this step might take some time depending on the GPU(s) you have and your system configuration. On a single NVIDIA TitanX it take about 1.25 hours to generate all the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather all JPG files in the test set as small batches\n",
    "files = glob.glob(os.path.join(IMAGE_DIR, \"*.jpg\"))\n",
    "ALL_FILES=[]\n",
    "_buffer = []\n",
    "for _idx, _file in enumerate(files):\n",
    "    if len(_buffer) == config.IMAGES_PER_GPU * config.GPU_COUNT:\n",
    "        ALL_FILES.append(_buffer)\n",
    "        _buffer = []\n",
    "    else:\n",
    "        _buffer.append(_file)\n",
    "\n",
    "if len(_buffer) > 0:\n",
    "    ALL_FILES.append(_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over all the batches and predict\n",
    "_final_object = []\n",
    "for files in tqdm.tqdm(ALL_FILES):\n",
    "    images = [skimage.io.imread(x) for x in files]\n",
    "    predoctions = model.detect(images, verbose=0)\n",
    "    for _idx, r in enumerate(predoctions):\n",
    "        _file = files[_idx]\n",
    "        image_id = int(_file.split(\"/\")[-1].replace(\".jpg\",\"\"))\n",
    "        for _idx, class_id in enumerate(r[\"class_ids\"]):\n",
    "            if class_id == 1:\n",
    "                mask = r[\"masks\"].astype(np.uint8)[:, :, _idx]\n",
    "                bbox = np.around(r[\"rois\"][_idx], 1)\n",
    "                bbox = [float(x) for x in bbox]\n",
    "                _result = {}\n",
    "                _result[\"image_id\"] = image_id\n",
    "                _result[\"category_id\"] = 100\n",
    "                _result[\"score\"] = float(r[\"scores\"][_idx])\n",
    "                _mask = maskUtils.encode(np.asfortranarray(mask))\n",
    "                _mask[\"counts\"] = _mask[\"counts\"].decode(\"UTF-8\")\n",
    "                _result[\"segmentation\"] = _mask\n",
    "                _result[\"bbox\"] = [bbox[1], bbox[0], bbox[3] - bbox[1], bbox[2] - bbox[0]]\n",
    "                _final_object.append(_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write prediction files to JSON and submit to crowdAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = open(\"predictions.json\", \"w\")\n",
    "import json\n",
    "print(\"Writing JSON...\")\n",
    "fp.write(json.dumps(_final_object))\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import crowdai\n",
    "api_key = \"YOUR-CROWDAI-API-KEY-HERE\"\n",
    "challenge = crowdai.Challenge(\"crowdAIMappingChallenge\", api_key)\n",
    "result = challenge.submit(\"predictions.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Author\n",
    "Sharada Mohanty [sharada.mohanty@epfl.ch](sharada.mohanty@epfl.ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
